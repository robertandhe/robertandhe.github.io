I"<h2 id="1-knn理论介绍">1. KNN理论介绍</h2>
<p> K近邻(k-nearest neighbors, KNN)是一种基本的机器学习方法，KNN既可以做分类，也可以做回归。
KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。KNN做分类预测时，一般选择多数表决法，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。而KNN做回归时，一般选择平均法，即最近的K个样本的输出的平均值作为回归预测值。这里主要介绍KNN分类方法。
可以看出KNN不具有显式的学习过程。KNN实际上利用训练数据集对特征向量空间进行划分。k值得选择、距离度量及分类决策规则是KNN的三个基本要素。</p>
<h3 id="knn算法三要素">KNN算法三要素</h3>
<ol>
  <li>k值选择<br />
 对于k值的选择，没有固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。<br />
 选择较小的k值，就相当于用较小的邻域中的训练实例进行预测，学习的近似误差会减小，只有与输入实例较近的训练实例才会对预测结果起作用。但缺点是学习的估计误差会增大，预测结果会对近邻的实例点非常敏感。如果邻近的实例点恰巧是噪声，预测就会出错。换句话说，k值的减小就意味着整体模型变得复杂，容易发生过拟合。<br />
 选择较大的k值，就相当于用较大邻域中的训练实例进行预测，其优点是可以减少学习的估计误差。但缺点是学习的近似误差会增大。这时候，与输入实例较远训练实例也会对预测起作用，使预测发生错误。k值得增大就意味着整体的模型变得简单。</li>
</ol>
:ET