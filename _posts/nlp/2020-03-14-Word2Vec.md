---
layout: post
title: "Word2Vec详解"
subtitle: ''
date:   2020-03-14 19:26:00
author: "mudux"
music: false
music-id: 187937
mathjax: true
# header-img: "img/post-bg-2015.jpg"
header-style: text
catalog: true
tags:
  - nlp
---

&emsp;&emsp;word2vec是google在2013年推出的一个NLP工具，它可以将所有的词向量化(word embedding)。word2vec可以在百万数量级的词典和上亿的数据集上进行高效的训练。这里总结下word2vec背后的算法模型。

# 1. 统计语言模型
&emsp;&emsp;NLP中的一个基本问题：如何计算一段文本序列在某种语言下出现的概率？它在很多NLP任务中都扮演着重要的角色。例如，在机器翻译问题中，如果知道了目标语言中每句话的概率，就可以从候选集合中挑选出最合理的句子作为翻译结果。  
&emsp;&emsp;统计语言模型给出了这一类问题的一个基本解决框架。对于一段文本序列

$$S=w_1,w_2,\cdots,w_T$$

&emsp;&emsp;它的概率可以表示为

$$P(S)=P(w_1,w_2,\cdots,w_T)=\prod\limits_{t=1}^Tp(w_t|w_1,w_2,\cdots,w_{t-1})$$

&emsp;&emsp;即将序列的联合概率转化为一系列概率的乘积。问题变成了如何去预测这些给定previous word下的条件概率：

$$p(w_t|w_1,w_2,\cdots,w_{t-1})$$

&emsp;&emsp;由于其巨大的参数空间，这一原始模型在实际中难以应用。更多的是采用其简化版本-Ngram模型：

$$p(w_t|w_1,w_2,\cdots,w_{t-1})\approx p(w_t|w_{t-n+1},\cdots,w_{t-1})$$

&emsp;&emsp;常见的如bigram(N=2)模型和trigram(N=3)模型。  
&emsp;&emsp;Ngram模型仍有局限性。首先，由于参数空间的爆炸式增长，它无法处理长的context。其次，它没有考虑词与词之间内在的联系性。所以希望能够表示词之间的相似性。  
&emsp;&emsp;之前的方法用one-hot向量表示词向量，每个词以$R^{|V|\times 1}$的向量表示，某个位置为1，其余位置为0，$|V|$为词典大小。例如：

$$w^{aardvark}=\left[
    \begin{array}{l}
    1\\
    0\\
    0\\
    \vdots\\
    0
    \end{array}
    \right],
    w^{a}=\left[
    \begin{array}{l}
    0\\
    1\\
    0\\
    \vdots\\
    0
    \end{array}
    \right],
    w^{at}=\left[
    \begin{array}{l}
    0\\
    0\\
    1\\
    \vdots\\
    0
    \end{array}
    \right],
    \cdots,
    w^{zebra}=\left[
    \begin{array}{l}
    0\\
    0\\
    0\\
    \vdots\\
    1
    \end{array}
    \right]
$$

&emsp;&emsp;这种方法每个词是完全独立的。如

$$\left(w^{hotel}\right)^Tw^{motel}=\left(w^{hotel}\right)^Tw^{cat}=0$$

&emsp;&emsp;所以可以把空间维度从
$$|V|$$
减小，在稠密子空间中编码词之间的关系。

# 2. 基于SVD的方法
&emsp;&emsp;这类方法首先遍历大规模数据集并累计词共现次数计入矩阵$X$，对矩阵$X$进行奇异值分解得到$USV^T$。矩阵$U$的行就是词嵌入，得到矩阵$X$的方法有几种。
## 2.1 Word-Document Matrix
&emsp;&emsp;这个方法假设在同一个文档中出现的词是相关的。例如"banks","bonds","stocks","money"可能同时出现，而"banks","octopus","banana"和"hockey"可能不会同时出现。所以可以遍历文本库，每次单词$i$在文本$j$中出现，$X_{ij}$加1，矩阵$X$的大小为$R^{|V|\times M}$，$M$为文本数量。$M$可能很大。
## 2.2 Windown based Co-occurrence Matrix
&emsp;&emsp;矩阵$X$存储共现词(co-occurrence word)，所以$X$是对称矩阵。在这个方法中特定词的定长窗口中每个词出现的次数。例如
![2.1](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-03-14/2.1.PNG)

## 2.3 Applying SVD to the cooccurrence matrix
&emsp;&emsp;对矩阵$X$进行奇异值分解，根据奇异值占比在特定位置$k$截断：

$$\frac{\sum\limits_{i=1}^k \sigma_i}{\sum\limits_{i=1}^{|V|} \sigma_i}$$

&emsp;&emsp;子矩阵$U_{1:|V|,1:k}$就是词嵌入矩阵，每个单词词向量的维度为$k$。  
&emsp;&emsp;但这些方法有几个缺点：
- 矩阵维度可能变化(加入新的词到字典，文本库改变)
- 矩阵稀疏(大多数词不会共现)
- 矩阵维度高
- 二次时间复杂度(奇异值分解)
- 词频不均衡

# 3. Neural Network Language Model(NNLM)
&emsp;&emsp;2003年，Bengio发表了神经网络语言模型(NNLM)论文，NNLM采用简单前向反馈神经网络来拟合一个词序列的条件概率$p(w_t|w_1,w_2,\cdots,w_{t-1})$,其网络结构如下图所示:

![3.1](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-03-14/3.1.PNG)

&emsp;&emsp;NNLM模型可以分成两部分理解：
- 线性的embedding层。将单词$w_t$的前$n-1$词的one-hot向量输入，通过一个共享的矩阵
  $$C\in d\times |V|$$
  ，映射为$n-1$个分布式的词向量。其中
  $$|V|$$
  为词典的大小，$d$为embedding向量的维度。$C$矩阵存储了要学习的word vector。
- 一个神经网络$g$。由一个tanh隐层和一个softmax输出层组成。将embedding层输出的$n-1$个词向量映射为一个长度为
  $$|V|$$
  的概率分布向量。

# 4. Word2Vec模型
&emsp;&emsp;基于SVD的方法需要存储全部信息，遍历大规模文本库，word2vec创建了一个简单的优化模型。word2vec主要任务目标是为了训练得到词向量，而不是NNLM的任务$(p(w_t|w_1,w_2,\cdots,w_{t-1}))$。word2vec包含了两个模型CBOW，Skip-gram。
## 4.1 Continuous Bag of Words Model(CBOW)
&emsp;&emsp;以"The cat jumped over the puddle"为例，中心词(cente word)取为"jumped"。CBOW把${"The","cat","over","the","puddle"}$当作上下文(context)，用这些上下文词去预测中心词("jumped")。  
&emsp;&emsp;已知参数为句子中单词的one-hot表示，用$x^{(c)}$表示输入上下文词one-hot向量，$y^{(c)}$表示输出，在CBOW模型中仅有一个输出，即中心词。  
&emsp;&emsp;创建两个矩阵，${\bf{V}}\in R^{n\times |V|},{\bf{U}}\in R^{|V|\times n}$，$n$表示词向量空间的大小。${\bf{V}}$是输入词矩阵，第$i$列是第$i$个单词$w_i$的$n$维词向量，用$v_{i}$表示。${\bf{U}}$是输出词矩阵，第$j$行是第$j$个单词的$n$维词向量，用$u_j$表示。所以其实对每个单词$w_i$同时学得的两个词向量。  
&emsp;&emsp;CBOW模型可以用下面的步骤描述：
1. 生成输入上下文词的one-hot向量，大小为$2m$
   $$x^{(c-m)},\cdots,x^{(c-1)},x^{(c+1)},\cdots,x^{(c+m)}\in \bf{R}^{|V|}$$

2. 得到上下文词嵌入词向量
   $$v_{c-m}=\bf{V}x^{(c-m)},v_{c-m+1}=\bf{V}x^{(c-m+1)},\cdots,v_{c+m}=\bf{V}x^{(c+m)}\in \bf{R}^n$$

3. 对嵌入词向量做平均
   $$\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+\cdots+v_{c+m}}{2m}\in \bf{R}^n$$

4. 得到打分向量
   $$z=\bf{U}\widehat{v}\in \bf{R}^{|V|}$$
   。因为相似向量的內积大，所以这个运算使相近词的內积有大的分数。
5. 通过softmax函数把得分转换为概率
   $$\widehat{y}=softmax(z)\in \bf{R}^{|V|}$$

6. 希望得到的概率向量
   $$\widehat{y}\in \bf{R}^{|V|}$$
   和真实概率向量
   $$y\in \bf{R}^{|V|}$$
   相匹配，这里$y$也是一个one-hot向量。

&emsp;&emsp;CBOW用交叉熵衡量$y$和$\widehat{y}$的差距，即使用交叉熵目标函数。

$$H(\widehat{y},y)=-\sum\limits_{j=1}^{|V|}y_j\log(\widehat(y))$$

&emsp;&emsp;因为这里$y$是中心词对应one-hot向量，假设$c$位置为1，所以上式可以化简

$$H(\widehat{y},y)=-y_c\log(\widehat(y)_c)$$

&emsp;&emsp;所以优化目标为：

$$
\begin{array}{lll}
minimize\ J
&=& -\log P(w_c|w_{c-m},\cdots,w_{c-1},w_{c+1},\cdots,w_{c+m})\\
&=& -\log P(u_c|\widehat(v))\\
&=& -\log \frac{exp(u_c^T\widehat{v})}{\sum\limits_{j=1}^{|V|}exp(u_j^T\widehat{v})}\\
&=& -u_c^T+\log \sum\limits_{j=1}^{|V|}exp(u_j^T\widehat{v})
\end{array}
$$

&emsp;&emsp;用SGD更新相应的词向量$u_c$和$v_j$。

## 4.2 Skip-Gram Model
&emsp;&emsp;以上面的例子为例，skip-gram在中心词"jumped"给定的情况下预测周围的词"The","cat","over","the","puddle"。  
&emsp;&emsp;skip-gram模型步骤和CBOW模型大体相似：
1. 得到中心词one-hot向量
   $$x\in \bf{R}^{|V|}$$
2. 得到中心词嵌入词向量
   $$v_c=\bf{V}x\in\bf{R}^{n}$$

3. 得到打分向量$z=\bf{U}v_c$
4. 通过softmax函数把打分向量转换为概率$\widehat{y}=softmax(z)$，注意这个向量中
   $$\widehat{y}_{c-m},\cdots,\widehat{y}_{c-1},\widehat{y}_{c+1},\cdots,\widehat{y}_{c+m}$$
   是每个上下文词的概率
5. 希望得到的概率向量和真实概率向量匹配$y^{(c-m)},\cdots,y^{(c-1)},y^{(c+1)},\cdots,y^{(c+m)}$是真实输出的one-hot向量

skip-gram的目标函数利用了朴素贝叶斯模型的假设，即条件概率独立：

$$
\begin{array}{lll}
minimize\ J
&=& -\log P(w_{c-m},\cdots,w_{c-1},w_{c+1},\cdots,w_{c+m}|w_c)\\
&=& -\log \mathop \Pi\limits_{j=0,j\ne m}^{2m}P(w_{c-m+j}|w_c)\\
&=& -\log \mathop \Pi\limits_{j=0,j\ne m}^{2m} P(u_{c-m+j}|v_c)\\
&=& -\log \mathop \Pi\limits_{j=0,j\ne m}^{2m} \frac{exp(u_{c-m+j}^Tv_c)}{\sum\limits_{k=1}^{|V|}exp(u_k^Tv_c)}\\
&=& -\sum\limits_{j=0,j\ne m}^{2m}u_{c-m+j}^Tv_c + 2m\log \sum\limits_{k=1}^{|V|}exp(u_k^Tv_c)\\
\end{array}
$$

&emsp;&emsp;注意这里可以表示为

$$
\begin{array}{lll}
J &=& -\sum\limits_{j=0,j\ne m}^{2m} \log P(u_{c-m+j}|v_c)\\
&=& \sum\limits_{j=0,j\ne m}^{2m} H(\widehat{y}, y_{c-m+j})
\end{array}
$$

&emsp;&emsp;$H(\widehat{y}, y_{c-m+j})$是概率向量$\widehat{y}$和one-hot向量$y_{c-m+j}$的交叉熵。  
&emsp;&emsp;上面两个模型其实可以用一个单隐层的神经网络来表示，如下图所示：
![4.1](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-03-14/4.1.PNG)

&emsp;&emsp;输入为one-hot表示词向量，输入到隐层的变换矩阵
$$\bf{V}\in \bf{R}^{n\times |V|}$$
为输入词矩阵，这两个相乘其实就是embedding-lookup，在$\bf{V}$中找到对应的列，其词嵌入。隐层到softmax层的变换矩阵
$$\bf{U}\in \bf{R}^{|V|\times n}$$
为输出词矩阵，即对得到的词嵌入和输出词矩阵两两做內积，计算相似性，再把这个打分向量转换为概率。所以输入矩阵$\bf{V}$和输出矩阵$\bf{U}$都是词嵌入矩阵，对每个单词有两个表示，最终词嵌入为对应向量的平均。而且$\bf{V}$和$\bf{U}$为神经网络的权值矩阵，所以我们训练这个模型的目标是为了得到权值矩阵。

# 5. 损失函数优化
&emsp;&emsp;在计算softmax时需要遍历词典，大小为$|V|$，这无疑是很耗时的，而且每次迭代都需要更新所有参数。word2vec提出了两个近似方法。
## 5.1 Negative Sampling
&emsp;&emsp;负采样在迭代每个训练样本时，只更新部分网络参数。以前面例子为例，在skip-gram模型下，考虑$(w,c)$词对，“jumped”为中心词，$m=2$,训练集包含$("jumped", "The"),("jumped", "fox"),("jumped", "over"),("jumped", "the")$。取$("jumped", "fox")$，这个训练样本来自于真实训练集，对它人为构造$K$个负样本，取$K=3$为例，可以构造$("jumped", "paper"),("jumped", "tomato"),("jumped", "bank")$，这些都不是来自于"jumped"对应的训练样本。用$P(D=1|w,c)$表示$(w,c)$来自于文本库构造的训练集，相应的$P(D=0|w,c)$表示$(w,c)$不是来自于文本库构造的训练集。目标是要区分$(w,c)$是不是来自文本库，用sigmoid函数拟合这个模型：

$$P(D=1|w,c,\theta)=\sigma(v_c^T,v_w)=\frac{1}{1+e^{(-v_c^Tv_w)}}$$

&emsp;&emsp;注意这里sigmoid模型参数为词矩阵$\bf{V}$和$\bf{U}$。

$$
\begin{array}{lll}
\theta &=& \arg\max\limits_{\theta} \mathop\Pi\limits_{(w,c)\in D}P(D=1|w,c,\theta)\mathop\pi\limits_{(w,c)\in \widetilde{D}}P(D=0|w,c,\theta)\\
&=& \arg\max\limits_{\theta} \mathop\Pi\limits_{(w,c)\in D}P(D=1|w,c,\theta)\mathop\Pi\limits_{(w,c)\in \widetilde{D}}(1-P(D=1|w,c,\theta))\\
&=& \arg\max\limits_{\theta} \sum\limits_{(w,c)\in D} \log P(D=1|w,c,\theta) + \sum\limits_{(w,c)\in \widetilde{D}}\log(1-P(D=1|w,c,\theta))\\
&=& \arg\max\limits_{\theta} \sum\limits_{(w,c)\in D} \log \frac{1}{1+exp(-u_w^Tv_c)} + \sum\limits_{(w,c)\in \widetilde{D}}\log(1-\frac{1}{1+exp(-u_w^Tv_c)})\\
&=& \arg\max\limits_{\theta} \sum\limits_{(w,c)\in D} \log \frac{1}{1+exp(-u_w^Tv_c)} + \sum\limits_{(w,c)\in \widetilde{D}}\log(\frac{1}{1+exp(u_w^Tv_c)})\\
\end{array}
$$

&emsp;&emsp;目标函数就是极小化负对数似然函数

$$J=-\sum\limits_{(w,c)\in D} \log \frac{1}{1+exp(-u_w^Tv_c)} - \sum\limits_{(w,c)\in \widetilde{D}}\log(\frac{1}{1+exp(u_w^Tv_c)})$$

&emsp;&emsp;注意这里$\widetilde{D}$是负样本。所以现在给定中心词$c$情况下，对上下文词$c-m+j$的新的目标函数为：

$$-\log\sigma(u_{c-m+j}^T v_c)-\sum\limits_{k=1}^K \log\sigma(-\widetilde{u}_k^T v_c)$$

&emsp;&emsp;同理对CBOW模型，在给定上下文词向量$\widehat{v}=\frac{v_{c-m}+v_{c-m+1}+\cdots+v_{c+m}}{2m}$情况下，中心词$u_c$的目标函数为

$$-\log\sigma(u_c^T\widehat{v})-\sum\limits_{k=1}^K\log\sigma(-\widehat{u}_k^T\widehat{v})$$


&emsp;&emsp;负样本的选取和频次相关，频次越高，负采样概率越大，经验公式为：

$$P(w_i)=\frac{f(w_i)^{\frac{3}{4}}}{\sum\limits_{j=1}^{|V|}(f(w_j)^{\frac{3}{4}})}$$

## 5.2 Hierarchical Softmax
&emsp;&emsp;见[hierarchical softmax](https://www.cnblogs.com/pinard/p/7243513.html)

# 6. 工程优化
## 6.1 高频词下采样
&emsp;&emsp;在大规模文本库中，高频词出现频率很高，同时提供的信息比低频词少。例如共现词"France"和"Paris"就比"France"和"the"有用许多，因为几乎所有词都和"the"共现。所以高频词的向量表示训练很多次也不会有大的改变。  
&emsp;&emsp;word2vec利用一个采样方法解决这个问题，每个词$w_i$以$P(w_i)$的概率被舍弃：

$$P(w_i)=1-\sqrt{\frac{t}{f(w_i)}}$$

&emsp;&emsp;$f(w_i)$是词频，$t$是设定阈值。
## 6.2 短语学习
&emsp;&emsp;短语例如"Boston Globe"不是单个词"Boston"和"Globe"意义的简单组合，所以需要对短语用新的表示方法。  
word2vec利用数据驱动方法寻找这些短语，下面公式

$$score(w_i,w_j)=\frac{count(w_iw_j)-\delta}{count(w_i)\times count(w_j)}$$

&emsp;&emsp;计算每个词组的分数决定是否放在训练集中。

# 7. Reference
[1 Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)  
[2 Distributed Representations of Words and Phrases and their Compositionality](http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)   
[3 word2vec tutorial](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)    
[4 word2vec CS224N note](https://web.stanford.edu/class/archive/cs/cs224n/cs224n.1194/readings/cs224n-2019-notes01-wordvecs1.pdf)  
[5 NLP之——Word2Vec详解](https://www.cnblogs.com/guoyaohua/p/9240336.html)  
[6 word2vec详解](https://nocater.github.io/2018/12/29/word2vec%E8%AF%A6%E8%A7%A3/)  
[7 word2vec原理(二) 基于Hierarchical Softmax的模型](https://www.cnblogs.com/pinard/p/7243513.html)