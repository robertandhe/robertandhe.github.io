---
layout: post
title: "logistic回归算法理论详解"
subtitle: ''
date:     2020-02-03 15:33:00
author: "mudux"
music: false
music-id: 187937
mathjax: true
# header-img: "img/post-bg-2015.jpg"
header-style: text
catalog: true
tags:
  - machine learning
---

>logistic regression(LR)回归从名字上看是回归算法，但其实是一个分类算法。是机器学习中最常用的算法之一。LR回归用来估计事物发生的概率，如用户购买商品的概率，广告被点击的概率。

## 1. 概率预测问题
&emsp;在线性判别模型如PLA下，我们直接预测测试样本是pass/fail, positive/negative，etc。假设现在想要预测事件发生的概率而不仅仅是发生或者不发生。但是在训练模型时，我们只有hard binary classification的数据集，因为数据标记只有发生或不发生而不是发生/不发生的概率，没有办法直接根据概率来建模。所以需要新的模型来解决概率预测模型。

## 2. 二元logistic回归模型
### 2.1 二元logistic分布
&emsp;Logistic分布是一种连续型的概率分布，其分布函数和密度函数分别为：

$$F(x) = P(X \le x) = \frac{1}{1 + e^{ - \frac{x - \mu }{\gamma }}}$$

$$f(x) = F'(X \le x) = \frac{e^{ -\frac{x - \mu}{\gamma}}}{\gamma ( 1 + e^{ - \frac{x - \mu}{\gamma }})^2}$$

&emsp;其中，$\mu$表示**位置参数**，$\gamma \gt 0$表示**形状参数**。其图像如下图所示：

![logistic distribution](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-02-03/logistic.png){:height="80%" width="80%"}

&emsp;Logistci分布是由其位置参数和形状参数定义的连续分布。Logistic分布的形状与正太分布的形状形似，但是Logistic分布的尾部更长。在深度学习中常用的Sigmoid函数就是Logistic分布在$\mu = 0, \gamma = 1$的特殊形式。

### 2.2 二元logisitc回归模型
&emsp;在统计学中，用对数模型(logit model)求事件发生的概率。事件的几率(odds)是$p=P({y=1|x,\theta})$和$p=P( {y=0|x,\theta})$的比值。假设对数几率是输入的线性函数，即

$$\log \frac{P\left({y = 1\left|x,\theta\right.}\right)}{1-P\left({y=1\left|x,\theta\right.}\right)} = \theta^T x$$

&emsp;由此可求出事件发生的概率为：

$$P\left( {y = 1\left| x,\theta \right.} \right) = \frac{\exp \left( {\theta^T x} \right)}{1 + \exp \left( {\theta^T x} \right)}$$

$$P\left( {y = 0\left| x,\theta \right.} \right) = \frac{1}{1 + \exp \left( {\theta^T x} \right)}$$

&emsp;转换可得：

$$P\left( {y = 1\left| x,\theta \right.} \right) = \frac{1}{1 + \exp \left( { - \theta^T x} \right)}$$

&emsp;上式即是logistic回归模型，就是一个sigmoid函数。

&emsp;通过上述推到可以看出logistic回归实际上是使用线性回归模型的预测值逼近分类任务真实标记的对数几率，其优点有：
- 直接对分类的概率建模，无需事先假设数据分布，从而避免了假设分布不准确带来的问题；
- 不仅可以预测出类别，还能得到该预测的概率，这对一些利用概率辅助决策的任务有用；
- 对数几率函数是任意阶可导的凸函数，有许多数值优化算法可以求出最优解。


&emsp;令$z=\theta^Tx$，logistic回归模型可表示为：

$$f(z)=\frac{1}{1+exp(-z)}=\frac{1}{1+exp(-\theta^Tx)}$$

&emsp;logisitc回归模型有个很好的导数性质：

$$f'(z)=f(z)(1-f(z))$$

### 2.3 二元logistic回归模型损失函数
>注：这里标签为0和1

&emsp;logistic回归模型的数学形式确定后，剩下的就是如何求解模型中的参数。在统计学中，常用极大似然估计法求解，即找到一组参数，使得在这组参数下，数据的似然度(概率)最大。  
&emsp;设：

$$P(y=1|x,\theta)=h_\theta(x)$$

$$P(y=0|x,\theta)=1-h_\theta(x)$$

&emsp;似然函数为：

$$L(\theta) = \mathop \Pi \limits_{i = 1}^m (h_\theta(x^{(i)}))^{y^(i)}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}}$$

&emsp;其中$m$为样本的个数。

&emsp;对数似然函数为：
$$
\begin{array}{c}
\ln L(\theta) = \sum\limits_{i = 1}^m {y^i\ln (h_\theta ( {x^{(i)}})) + (1 - y^{(i)})\ln (1 - h_\theta (x^{(i)}))} \\
 = \sum\limits_{i = 1}^m y^{(i)}{\theta ^T}x^{(i)}  - \ln ({1 + \exp (\theta ^T}x^{(i)}))
\end{array}
$$



&emsp;对似然函数对数化取反的表达式，即损失函数表达式为：

$$J(\theta) =  - \ln L(\theta) =  - \sum\limits_{i = 1}^m y^{(i)}\ln( {h_\theta (x^{(i)})) + ({1 - y^{(i)}})\ln (1 - h_\theta (x^{(i)}))} $$


&emsp;用矩阵法表示损失函数：

$$J\left( \theta  \right) =  - {Y^T}\ln {h_\theta }\left( X \right) - {\left( {I - Y} \right)^T}\ln \left( {I - {h_\theta }\left( X \right)} \right)$$

### 2.4 二元logistic回归模型损失函数优化方法
>注：这里标签为0和1

&emsp;对于二元logistic回归的损失函数极小化，常见的方法有梯度下降法、坐标轴下降法，牛顿法等。
1. 随机梯度下降法

&emsp;梯度下降是通过$J(\theta)$对$\theta$的一阶导数来找下降方向，并且以迭代的方式来更新参数，更新方式为：

$$g_j = \frac{\partial J(\theta)}{\partial \theta_j} = ({h_\theta ({x^{(i)}})-{y^{(i)}}})x^{(i)}$$

$$\theta _j^{k + 1} = \theta _j^k - \alpha {g_j}$$

&emsp;其中，$\alpha$为学习率。

&emsp;矩阵法推导二元logistic回归梯度的过程为：
&emsp;对于

$$J(\theta) =  - {Y^T}\ln h_\theta(X) - {(I - Y)^T}\ln ( {I - h_\theta(X)})$$

&emsp;用$J(\theta)$对$\theta$向量求导可得：


$$\frac{\partial }{{\partial \theta }}J(\theta) = X^T \left[ {\frac{1}{h_\theta (X)} \odot h_\theta (X) \odot (I - h_\theta (X)) \odot (- Y)} \right] + {X^T}\left[ {\frac{1}{I - h_\theta (X)} \odot h_\theta (X) \odot \left( {I - h_\theta (X)} \right) \odot (I - Y)} \right]$$


&emsp;化简可得：

$$\frac{\partial}{\partial \theta}J(\theta)=X^T(h_\theta(X)-Y)$$

&emsp;从而在梯度下降法中每一步向量$\theta$的迭代公式如下：

$$\theta=\theta-\alpha X^T(h_\theta(X)-Y)$$

2. 牛顿法

&emsp;牛顿法的基本思路是，在现有极小点估计值的附近对$f(x)$做二阶泰勒展开，进而找到极小点的下一个估计值。假设$\theta^k$为当前的极小值估计值，那么有：

$$\varphi \theta = J(\theta ^k) + {J'}(\theta ^k)(\theta - \theta ^k) + \frac{1}{2}J^{''}({\theta ^k}){({\theta - \theta ^k})^2}$$

&emsp;然后令$\varphi'(w)=0$，得到了$\theta^{(k+1)}=\theta^{k}-\frac{J'(\theta^{k})}{J''(\theta^{k})}$，因此有迭代更新式：

$$\theta^{(k+1)}=\theta^{k}-\frac{J'(\theta^{k})}{J''(\theta^{k})}=\theta^{k}-H_k^{-1}{g_k}$$

&emsp;其中，$H_k^{-1}$为海森矩阵。

### 2.5 二元logistic回归模型的正则化

&emsp;正则化是一个通用的算法和思想，用来避免过拟合。在经验风险最小化(训练误差最小化)的基础上，尽可能采用简单的模型，可以有效提高泛化预测精度。如果模型过于复杂，输入变量稍有变动就会引起输出大变化，影响预测精度。正则化之所以有效，就在于其降低了特征的权重，使得模型更为简单。

&emsp;正则化一般采用L1范式或者L2范式，其形式分别为$\phi(\theta){\rm{ = }}{\left\| \theta  \right\|_1}$和$\phi(\theta){\rm{ = }}{\left\| \theta  \right\|_2}$。


1. L1正则化  
&emsp;L1正则化相当于为模型添加了这样一个先验知识：$\theta$服从零均值拉普拉斯分布。
拉普拉斯分布为：

$$f(\theta | \mu, b)=\frac{1}{2b}exp(-\frac{|\theta-\mu|}{b})$$

&emsp;引入先验知识后，似然函数为：

$$
\begin{array}{c}
L(\theta) = P(y|x,\theta)P(\theta)\\
 = \mathop \Pi \limits_{i = 1}^m p(x^{(i)})^{y^{(i)}}(1 - p(x^{(i)}))^{1 -{y^{(i)}}}\mathop \Pi \limits_{j = 1}^d \frac{1}{2b}\exp (- \frac{\left| \theta  \right|}{b})
\end{array}
$$



&emsp;取对数再取负，得到目标函数：

$$\rm{-}\ln L(\theta) =  - \sum\limits_{i = 1}^m {y^{(i)}\ln ( h_\theta (x^{(i)})) + (1 - y^{(i)})\ln ( 1 - {h_\theta }( x^{(i)}))}  + \frac{1}{2{b^2}}\sum\limits_{j = 1}^d |\theta _j| $$

&emsp;等价于原始损失函数的后面加上了L1正则，因此L1正则的本质其实是为模型增加了**模型参数服从零均值拉普拉斯分布**这一先验知识。

2. L2正则化

&emsp;L2正则化相当于为模型添加了这样一个先验知识：$\theta$服从零均值正态分布。

$$
\begin{array}{c}
L(\theta) = P(y|x,\theta)P(\theta)\\
 = \mathop \Pi \limits_{i = 1}^m h_\theta (x^{(i)})^{y^{(i)}}(1 - h_\theta(x^{(i)}))^{1 - y^{(i)}}\mathop \Pi \limits_{j = 1}^d \frac{1}{\sqrt {2\pi} \sigma}\exp ( -\frac{\theta_j^2}{2{\sigma ^2}})\\
 = \mathop \Pi \limits_{i = 1}^m h_\theta{(x^{(i)})^{y^{(i)}}}(1 - h_\theta(x^{(i)}))^{1 - {y^{(i)}}}\frac{1}{\sqrt {2\pi } \sigma}\exp (  - \frac{\theta ^T\theta}{2{\sigma ^2}})
\end{array}
$$


&emsp;取对数再取负，得到目标函数：

$$\rm{-}\ln L(\theta) =  - \sum\limits_{i = 1}^m {y^{(i)}} \ln (h_\theta(x^{(i)})) + (1 - y^{(i)})\ln ( 1 - h_\theta(x^{(i)})) + \frac{1}{2{\sigma ^2}}{\theta ^T}\theta $$

&emsp;等价于原始损失函数后面加上了L2正则，因此L2正则的本质其实就是为模型增加了**模型参数服从零均值正态分布**这一先验知识。


## 3. 多元logistic回归模型

&emsp;前面局限于二元logistic回归，实际上二元logistic回归模型和损失函数容易推广到多元logistic回归。比如OVR和OVO。这里介绍多元logistic回归的softmax回归推导：  
&emsp;假设是$K$元分类模型，即输出$y$的取值为$1,2,...,K$。

&emsp;有：

$$\ln \frac{P(y = 1| x,\theta)}{P(y = K|x,\theta)} = \theta _1^Tx$$

$$\ln \frac{P(y = 2|x,\theta)}{P(y = K|x,\theta)} = \theta _2^Tx$$

$$...$$

$$\ln \frac{P(y = K-1|x,\theta)}{P(y = K|x,\theta)} = \theta _{K-1}^Tx$$

&emsp;概率之和为1：

$$\sum\limits_{i = 1}^K {P(y = i|x,\theta)}  = 1$$

&emsp;总共$K$个方程，$K$个变量。求解这个$K$元一次方程，得到$K$元logistic回归的概率分布为：

$$P(y = k|x,\theta) = \frac{\exp (\theta _k^Tx)}{1 + \sum\limits_{i = 1}^{K - 1} {\exp (\theta _i^Tx)}},k = 1,2,...,K - 1$$

$$P(y = K|x,\theta) = \frac{1}{1 + \sum\limits_{i = 1}^{K - 1} {\theta _i^Tx}}$$

&emsp;损失函数和优化方法与二元logistic回归类似。

## 4. logistic回归模型与其它模型的对比
### 4.1 与线性回归

&emsp;logistic回归是在线性回归的基础上加了一个sigmoid映射，本质上来说两者都属于广义线性模型。logistic回归模型解决分类问题，输出是离散值，线性回归解决的是回归问题，输出是连续值。

&emsp;sigmoid函数作用：
- 线性回归是在实数域范围内进行预测，而分类范围则需要在[0, 1]，logistic回归减少了预测范围；
- 线性回归在实数域上敏感度一致，而logistic回归在0附近敏感，在远离0点位置不敏感，使模型更加关注分类边界，增强模型鲁棒性。

### 4.2 与SVM

相同点：
- 都是分类算法，本质上都是在找最佳分类超平面；
- 都是监督学习算法；
- 都是判别式模型；
- 都可以增加不同的正则项。

不同点：
- Logistic是一个统计的方法，SVM是一个几何的方法；
- SVM只考虑支持向量，也就是和分类最相关的少数点去学习分类器。而Logistic通过非线性映射减小了离分类超平面较远的点的权重，相对提升了与分类最相关的数据点的权重；
- 损失函数不同：Logistic的损失函数是交叉熵，SVM的损失函数是HingeLoss，这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。对HingeLoss来说，其零区域对应的正是非支持向量的普通样本，从而所有的普通样本都不参与最终超平面的决定，这是支持向量机的最大优势所在，对训练样本数目的依赖减少，从而提高了训练效率；
- Logistic是参数模型，SVM是非参数模型，参数模型的前提是假设数据服从某一分布，该分布由一些参数确定（比如正太分布由均值和方差确定），在此基础上构建的模型称为参数模型；非参数模型对于总体的分布不做任何假设，只是知道总体是一个随机变量，其分布是存在的（分布中也可能存在参数），但是无法知道其分布的形式，更不知道分布的相关参数，只有在给定一些样本的条件下，能够依据非参数统计的方法进行推断。所以 Logistic 受数据分布影响，尤其是样本不均衡时影响很大，需要先做平衡，而 SVM 不直接依赖于分布；
- Logistic可以产生概率，SVM不能；
- Logistic不依赖样本之间的距离，SVM是基于距离的；
- Logistic相对来说模型更简单好理解，特别是大规模线性分类时并行计算比较方便。而 SVM 的理解和优化相对来说复杂一些，SVM 转化为对偶问题后，分类只需要计算与少数几个支持向量的距离，这个在进行复杂核函数计算时优势很明显，能够大大简化模型和计算；
- Logistic是经验风险最小化，需要另外加正则项，SVM自带结构风险最小化不需要加正则项。
- Logistic一般不用核函数，所以每两个点之间都要做內积，而SVM只需要计算样本和支持向量的內积，计算量更下。

### 4.3 与朴素贝叶斯

相同点：
- 都是分类模型

不同点：
- Logistic是判别式模型
$$P(y|x, \theta)$$
，朴素贝叶斯是生成式模型
$$P(x, y)$$
：判别式模型估计的是条件概率分布，给定观测变量$x$和目标变量$y$的条件模型，由数据直接学习决策函数$y=f(x)$或者条件概率分布$P(y|x, \theta)$作为预测的模型。判别方法关心的是对于给定的输入$x$，应该预测什么样的输出$y$。生成式模型估计的是联合概率分布，基本思想是首先建立样本的联合概率密度模型$P(x,y)$，然后再得到后验概率$P(y|x)$，利用它们进行分类，生成式模型更关心对于给定输入$x$和输出$y$的生成关系；
- NB的前提是特征条件独立，如果数据不符合这个情况，NB的表现就不好。


## 5. 模型细节

### 5.1 为什么Logistic适合离散特征

&emsp;在工业界，很少直接将连续值作为特征喂给Logistic回归模型，而是将连续特征离散化为一系列0，1特征交给Logistic回归模型，优势有：
- 稀疏向量內积乘法运算速度快，计算结果方便存储，容易扩展；
- 离散化后的特征对异常数据具有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；
- Logistic回归属于广义线性模型，表达能力受限，单变量离散化为N个1后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；
- 离散化后可以进行特征交叉，有M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；
- 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。


&emsp;李沐少帅指出，模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。

### 5.2 为什么损失函数不用MSE

&emsp;假设目标函数是MSE，即

$$L(\theta)=\frac{(y-h_\theta(x))^2}{2}$$

$$\frac{\partial L}{\partial \theta}=(h_\theta(x)-y)h_\theta(x)(1-h_\theta(x))x$$

&emsp;根据$\theta$的初始化，导数值可能很小而导致收敛变慢，训练过程中也可能因为该值过小而提前终止训练（梯度消失）。

&emsp;另一方面，交叉熵的梯度如下，当模型输出概率偏离真实概率是，梯度较大，加快训练速度，当拟合值接近真实概率时训练速度变缓慢，没有MSE的问题。

$$\frac{\partial L}{\partial \theta} = \sum\limits_{i = 1}^m {x^{(i)}({h_\theta }( {x^{(i)}}) - y^{(i)}}) $$


## 6. 标签为1/-1时二元logistic回归模型
>注：这里输出标签为1/-1。

&emsp;首先logistic回归为：

$$h_\theta(x)=\frac{1}{1+exp(-\theta^Tx)}$$

&emsp;表示输入$x$对应输出为1的概率。

&emsp;logistic分布有一个性质：

$$1-h_\theta(x)=h_\theta(-x)$$

&emsp;标签为1/-1时，似然函数为：

$$L(\theta)=\mathop \Pi \limits_{i = 1}^m h_\theta(y^{(i)}x^{(i)})$$

&emsp;交叉熵损失为：

$$ J(\theta)=\frac{1}{m}\sum\limits_{i = 1}^m -\ln h(y^{(i)}\theta ^Tx^{(i)})$$

&emsp;等价于：

$$ J(\theta)=\frac{1}{m}\sum\limits_{i = 1}^m {\ln \left( {1 + \exp \left( { - {y^{\left( i \right)}}{\theta ^T}{x^{\left( i \right)}}} \right)} \right)} $$

&emsp;上式即为**交叉熵损失**(cross-entropy loss)。


&emsp;损失函数梯度为：

$$\frac{\partial J(\theta)}{\partial \theta} = \frac{1}{m}\sum\limits_{i = 1}^m {h(- y^{(i)}{\theta ^T}x^{(i)})(- y^{(i)}x^{(i)})} $$

## 7. 后续

logistic并行化

## 8. Reference

[【机器学习】逻辑回归（非常详细）](https://zhuanlan.zhihu.com/p/74874291)  
[逻辑回归LR的特征为什么要先离散化](https://blog.csdn.net/yang090510118/article/details/39478033)    
[Logistic regression-wiki](https://en.wikipedia.org/wiki/Logistic_regression)  
[Logistic Regression (LR) 详解](https://blog.csdn.net/songbinxu/article/details/79633790#%E5%9B%9Blr-%E6%AD%A3%E5%88%99%E5%8C%96)  
统计学习方法第六章-李航  
机器学习第三章-周志华