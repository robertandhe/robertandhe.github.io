---
layout: post
title: "Adaboost算法理论详解与实现"
subtitle: ''
date:   2020-02-29 12:45:00
author: "mudux"
music: false
music-id: 187937
mathjax: true
# header-img: "img/post-bg-2015.jpg"
header-style: text
catalog: true
tags:
  - machine learning
---
&emsp;&emsp;前面介绍了Bagging方法中代表性的Random Forest方法，这里介绍Boosting中最具代表性的AdaBoost算法。   
&emsp;&emsp;AdaBoost算法包括AdaBoost分类算法和AdaBoost回归算法，如AdaBoost R2等。这里仅介绍AdaBoost分类算法。

# 1. AdaBoost算法
&emsp;&emsp;假设给定一个二类分类的训练数据集

$$T=\left\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\right\}$$

&emsp;&emsp;其中，每个样本点由实例与标记组成。实例$x_i\in \subseteq R^n$，标记$$y_i\in Y=\left\{-1,+1\right\}$$，$X$是实例空间，$y$是标记集合。AdaBoost利用以下算法，从训练数据中学习一系列弱分类器或基分类器，并将这些弱分类器线性组合成为一个强分类器。  
&emsp;&emsp;**算法1.1 (AdaBoost)**:  
&emsp;&emsp;输入：训练数据集$$T=\left\{(x_1,y_1),(x_2,y_2),\cdots,(x_N,y_N)\right\}$$，其中，$x_i\in \subseteq R^n$，标记$$y_i\in Y=\left\{-1,+1\right\}$$；弱学习算法；  
&emsp;&emsp;输出：最终分类器$G(x)$。
1. 初始化训练数据的权值分布
   
   $$D_1=(w_{11},\cdots,w_{1i},\cdots,w_{1N}),\ w_{1i}=\frac{1}{N},\ i=1,2,\cdots,N$$

2. 对$m=1,2,\cdots,M$
   - 使用具有权值分布$D_m$的训练数据集学习，得到基本分类器

        $$G_m(x):X\rightarrow\left\{-1,+1\right\}$$

   - 计算$G_m(x)$ 在训练数据集上的分类误差率

        $$e_m=P(G_m(x_i)\ne y_i)=\sum\limits_{i=1}^Nw_{mi}I(G_m(x_i)\ne y_i)\tag{1.1}$$
   - 计算$G_m(x)$的系数
        $$\alpha_m=\frac{1}{2} \log \frac{1-e_m}{e_m}\tag{1.2}$$

        这里的对数是自然对数。
   - 更新训练数据集的权值分布

        $$D_{m+1}=(w_{m+1,1},\cdots, w_{m+1,i},\cdots,w_{m+1,N})\tag{1.3}$$
    
        $$w_{m+1,i}=\frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)),\ i=1,2,\cdots,N\tag{1.4}$$
    
        这里，$Z_m$是规范化因子

     $$Z_m=\sum\limits_{i=1}^Nw_{mi}exp(-\alpha_my_iG_m(x_i))\tag{1.5}$$

        它使$D_{m+1}$成为一个概率分布。

3. 构建基本分类器的线性组合
    $$f(x)=\sum\limits_{m=1}^M\alpha_mG_m(x)\tag{1.6}$$

    得到最终分类器

    $$G(x)=sign(f(x))=sign\left(\sum\limits_{i=1}^M\alpha_mG_m(x)\right)\tag{1.7}$$

式(1.4)可以写成：

$$
w_{m+1,i}=\left\{
\begin{array}{ll}
\frac{w_{mi}}{Z_m}e^{-\alpha_m}, &G_m(x_i)=y_i\\
\frac{w_{mi}}{Z_m}e^{\alpha_m}, &G_m(x_i)\ne y_i
\end{array}
\right.
$$

&emsp;&emsp;由此可知，被基分类器$G_m(x)$误分类样本的权值得以扩大，而被正确分类样本的权值得以缩小，两相比较，误分类样本的权值被放大$e^{2\alpha_m}=\frac{1-e_m}{e_m}$倍。因此，五分类样本在下一轮学习中起更大的作用。不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基分类器的学习中起不同的作用，这时AdaBoost的一个特点。  


# 2. AdaBoost算法训练误差分析
&emsp;&emsp;**定理2.1 (AdaBoost的训练误差界)**   AdaBoost算法最终分类器的训练误差界为

$$\frac{1}{N}\sum\limits_{i=1}^N I(G(x_i)\ne y_i) \le \frac{1}{N}\sum\limits_{i}exp(-y_if(x_i))=\prod\limits_mZ_m$$

&emsp;&emsp;**定理2.2 (二分类问题AdaBoost的训练误差界)**

$$\prod\limits_{m=1}^M Z_m= \prod\limits_{m=1}^M\left[2\sqrt{e_m(1-e_m)}\right]=\prod\limits_{m=1}^M\sqrt{(1-4\gamma_m^2)}\le exp\left(-2\sum\limits_{i=1}^M\gamma_m^2\right)$$

&emsp;&emsp;这里，$\gamma_m=\frac{1}{2}-e_m$.

# 3. AdaBoost算法的解释
&emsp;&emsp;可认为AdaBoost算法是模型为加法模型、损失函数是指数函数、学习算法为前向分布算法时的二类分类学习方法。
## 3.1 前向分布算法
&emsp;&emsp;考虑加法模型(additive model)

$$f(x)=\sum\limits_{m=1}^M\beta_mb(x;\gamma_m)\tag{3.1}$$

&emsp;&emsp;其中，$b(x;\gamma_m)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。  
&emsp;&emsp;在给定训练数据及损失函数$L(y,f(x))$的条件下，学习加法模型$f(x)$成为经验风险最小化即损失函数极小化问题：

$$\mathop{\min}\limits_{\beta_m,\gamma_m}\sum\limits_{i=1}^N L\left(y_i,\sum\limits_{m=1}^M \beta_mb(x_i;\gamma_m)\right)\tag{3.2}$$

&emsp;&emsp;通常这是一个复杂的优化问题。前向分布算法(forward stagewise algorithm)求解这一优化问题的想法是：因为学习的是加法模型，如果能够从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式(3.2)，那么就可以简化优化的复杂度。具体地，每步只需优化如下损失函数：

$$\mathop{\min}\limits_{\beta,\gamma}\sum\limits_{i=1}^N L(y_i,\beta b(x_i;\gamma))\tag{3.3}$$

## 3.2 前向分布算法与AdaBoost
&emsp;&emsp;**定理3.1**: AdaBoost算法是前向分步加法算法的特例。这时，模型是由基本分类器组成的加法模型，损失函数是指数函数。  

&emsp;&emsp;**接下来证明**：  
&emsp;&emsp;当基函数是基本分类器时，该加法模型等价于AdaBoost的最终分类器

$$f(x)=\sum\limits_{i=1}^M\alpha_mG_m(x)$$

&emsp;&emsp;由基本分类器$G_m(x)$及其系数$\alpha_m$组成，$m=1,2,\cdots,M$。前向分布算法逐一学习基函数，这一过程与AdaBoost算法逐一学习基本分类器的过程一致。  
&emsp;&emsp;当前向分布算法的损失函数是指数损失函数

$$L(y,f(x))=exp[-yf(x)]$$

&emsp;&emsp;假设经过$m-1$轮迭代前向分步算法已经得到$f_{m-1}(x)$:

$$
\begin{array}{lll}
f_{m-1}(x) &=& f_{m-2}(x)+\alpha_{m-1}G_{m-1}(x)\\
&=& \alpha_1G_1(x)+\cdots+\alpha_{m-1}G_{m-1}(x)
\end{array}
$$

&emsp;&emsp;在第$m$轮迭代得到$\alpha_m,G_m(x)$和$f_m(x)$.

$$f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)$$

&emsp;&emsp;目标是使前向分步算法得到的$\alpha_m$和$G_m(x)$使$f_m(x)$在训练数据集$T$上的指数损失最小，即

$$(\alpha_m,G_m(x))=\arg\min\limits_{\alpha,G}\sum\limits_{i=1}^N exp\left[-y_i(f_{m-1}(x_i))+\alpha G(x_i)\right]\tag{3.4}$$

&emsp;&emsp;式(3.4)可以表示为：

$$(\alpha_m,G_m(x))=\arg\min\limits_{\alpha,G}\sum\limits_{i=1}^N {\overline w _{mi}}exp\left[-y_i\alpha G(x_i)\right]\tag{3.5}$$

&emsp;&emsp;其中，$$\overline w _{mi}=exp[-y_i f_{m-1}(x_i)]$$。因为$\overline w_{mi}$即不依赖于$\alpha$也不依赖于$G$，所以与最小化无关。但需注意$\overline w_{mi}$依赖于$f_{m-1}(x)$，随着每一轮迭代而发生改变。  
&emsp;&emsp;现证使式(3.5)达到最小的$$\alpha_m^{*}$$和$$G_m^{*}$$就是AdaBoost算法所得到的$$\alpha_m$$和$$G_m(x)$$，求解式(3.5)分两步：
首先，求$G_m^{*}(x)$。对任意$\alpha \gt 0$，使式(3.5)最小的$G(x)$由下式得到：

$$G_m^{*}(x)=\arg\min\limits_{G}\sum\limits_{i=1}^N {\overline w_{mi}}I(y_i\ne G(x_i))$$

&emsp;&emsp;此分类器$$G_m^{*}(x)$$即为AdaBoost算法的基分类器$G_m(x)$，因为它是使第$m$轮加权训练数据分类误差率最小的基本分类器。  
&emsp;&emsp;之后，求$$\alpha_m^{*}$$。

$$
\begin{array}{ll}
\sum\limits_{i=1}^N &{\overline w_{mi}}exp[-y_i\alpha G(x_i)]\\
&=\sum\limits_{y_i = G_m(x_i)}{\overline w_{mi}}e^{-\alpha} + \sum\limits_{y_i\ne G_m(x_i)}{\overline w_{mi}}e^{\alpha}\\
&=(e^{\alpha}-e^{-\alpha})\sum\limits_{i=1}^N {\overline w_{mi}} I(y_i\ne G(x_i)) + e^{-\alpha}\sum\limits_{i=1}^N{\overline w_{mi}}\tag{3.6}
\end{array}
$$

&emsp;&emsp;将已求得的$G_m^{*}(x)$代入式(3.6)，对$\alpha$求导并使导数为0，得到

$$\alpha_m^{*}=\frac{1}{2}\log\frac{1-e_m}{e_m}$$

&emsp;&emsp;其中，$e_m$是分类误差率：

$$e_m=\frac{\sum\limits_{i=1}^N{\overline w_{mi}} I(y_i\ne G_m(x_i))}{\sum\limits_{i=1}^N{\overline w_{mi}}}=\sum\limits_{i=1}^N w _{mi} I(y_i\ne G_m(x_i))$$

&emsp;&emsp;这里的$\alpha_m^{*}$和AdaBoost的$\alpha_m$完全一致。  
&emsp;&emsp;最后来看每一轮样本权值的更新。由 

$$f_m(x)=f_{m-1}(x)+\alpha_mG_m(x)$$

&emsp;&emsp;以及${\overline w_{mi}}=exp[-y_if_{m-1}(x_i)]$，可得

$$
{\overline w_{m+1,i}}={\overline w_{mi}}exp[-y_i\alpha_mG_m(x)]
$$

&emsp;&emsp;这和AdaBoost算法的样本权值更新等价，只差规范化因子。

# 4. AdaBoost算法的正则化
&emsp;&emsp;为了防止AdaBoost过拟合，通常也会加入正则化项，这个正则化项通常称为步长(learning rate)。定义为$v$，对于前面的弱学习器的迭代

$$f_k(x)=f_{k-1}(x) + \alpha_kG_k(x)$$

&emsp;&emsp;如果加上了正则化项，则有

$$f_k(x)=f_{k-1}(x) + v\alpha_kG_k(x)$$

&emsp;&emsp;$v$的取值范围为$0 \lt v \le 1$。对于同样的训练集学习效果，较小的$v$意味着需要更多的弱学习器的迭代。通常用步长和迭代最大次数一起决定算法的拟合效果。

# 5. AdaBoos小结
&emsp;&emsp;AdaBoost的主要优点有：
- AdaBoost作为分类器时，分类精度很高；
- 在AdaBoost框架下，可以使用各种回归分类器作为弱学习器，非常灵活；
- 作为简单的二元分类器时，构造简单，结果可理解；
- 不容易发生过拟合。

&emsp;&emsp;AdaBoost的主要缺点有：
- 对异常样本敏感，异常样本在迭代中可能会获得较高单位权重，影响最终的强学习器的预测准确性。

# 6. python代码实现
&emsp;&emsp;AdaBoost.py文件
```python

import numpy as np

class AdaBoostClassifier():
    def __init__(self, max_iter=50, verbose=False):
        self.max_iter = max_iter
        self.verbose = verbose

    def stumpClassify(self, data, dimen, threshVal, threshIneq):
        '''
        @description: decision stump classifier with given dimension
        @param : data - training data
                 dimen - split dimension
                 threshVal - split value
                 threshIneq - greater is 1 or lower is 1
        @return: predict label
        '''
        retArray = np.ones((np.shape(data)[0], 1))
        if threshIneq == 'lt':
            retArray[data[:, dimen] <= threshVal] = -1.0
        else:
            retArray[data[:, dimen] > threshVal] = -1.0
        return retArray
    
    def buildStump(self, data, classLabel, weight):
        '''
        @description: decision stump classifier which choose the best split dimension and split
                      value along all dimension
        @param:  data - training data features
                 classLabel - training data labels
                 weight - weigh array of data instance
        @return: bestStump - the dictionary save the best split dimension, split value, split direction and the alpha value of base classifier
                 minError - minimum predict error
                 bestClasEst - best predict labels
        '''
        m, n = np.shape(data)
        numSteps = 10
        bestStump = {}
        bestClasEst = np.zeros((m, 1))
        minError = np.inf
        for i in range(n):
            rangeMin = data[:, i].min()
            rangeMax = data[:, i].max()
            stepSize = (rangeMax - rangeMin) / numSteps
            for j in range(-1, int(numSteps) + 1):
                for inequal in ['lt', 'gt']:
                    threshVal = (rangeMin + float(j) * stepSize)
                    predictVals = self.stumpClassify(data, i, threshVal, inequal)
                    errArr = np.ones((m, 1))
                    errArr[predictVals == classLabel] = 0
                    weightedError = np.dot(weight.T, errArr)
                    if self.verbose:
                        print("split dim: %d, thresh: %.2f, thresh inequal: %s,\
                            the weighted error: %.3f" % (i, threshVal, inequal, weightedError))
                    if weightedError <= minError:
                        minError = weightedError
                        bestClasEst = predictVals.copy()
                        bestStump['dim'] = i
                        bestStump['thresh'] = threshVal
                        bestStump['ineq'] = inequal
        return bestStump, minError, bestClasEst
    
    def fit(self, data, classLabel):
        '''
        @description: fit function of Adaboost classifier
        @param: data - training data features
                classLabel - training data labels
        @return: all the base learner
        '''
        if not isinstance(data, (np.ndarray, np.generic)):
            data = np.array(data)
        if not isinstance(classLabel, (np.ndarray, np.generic)):
            classLabel = np.array(classLabel)
        self.weakClassArr = []
        m, _ = np.shape(data)
        weight = np.ones((m, 1)) / m
        aggClassEst = np.zeros((m, 1))
        for i in range(self.max_iter):
            if self.verbose:
                print("weight:", weight)            
            bestStump, error, classEst = self.buildStump(data, classLabel, weight)
            alpha = 0.5 * np.log((1.0 - error) / max(error, 1e-16))
            bestStump['alpha'] = alpha
            self.weakClassArr.append(bestStump)
            if self.verbose:
                print("classEst:", classEst.T)
            expon = -alpha * classEst * classLabel
            weight = weight * np.exp(expon)
            weight /= weight.sum()
            aggClassEst += alpha * classEst
            if self.verbose:
                print("aggClassEst:", aggClassEst.T)
            aggErrors = np.sign(aggClassEst) != classLabel
            errorRate = aggErrors.sum() / m
            if self.verbose:
                print("total error:", errorRate)
            if errorRate == 0:
                break
        return self.weakClassArr

    def predict(self, data):
        '''
        @description: prediction the label of test data
        @param : data - test data features
        @return: predicted labels of test data
        '''
        m = np.shape(data)[0]
        aggClassEst = np.zeros((m, 1))
        for i in range(len(self.weakClassArr)):
            classEst = self.stumpClassify(data, self.weakClassArr[i]['dim'], \
                self.weakClassArr[i]['thresh'], self.weakClassArr[i]['ineq'])
            aggClassEst += self.weakClassArr[i]['alpha'] * classEst
        if self.verbose:
            print("predict aggClassEst:", aggClassEst)                
        self.labels_ = np.sign(aggClassEst)
        self.labels_[np.where(self.labels_ == 0)] = -1
        return self.labels_

    def score(self, data, labels):
        '''
        @description: predict the label of test data and calculate the error rate
        @param: data - test data features
                labels - test data labels
        @return: accuracy
        '''
        pred = self.predict(data)
        acc = np.mean(pred == labels)
        if self.verbose:
            print("Accuracy:", acc)
        return acc
```

**测试一下**：
```python
import numpy as np
from sklearn.datasets import load_iris
from sklearn.preprocessing import scale
from sklearn.model_selection import train_test_split
from AdaBoost import AdaBoostClassifier

iris = load_iris()
data = iris['data']
target = iris['target']
m = data.shape[0]
x = scale(data)
y = np.zeros((m, 1))
y[target == 1] = 1
y[target != 1] = -1

trainX, testX, trainY, testY = train_test_split(x, y, test_size=0.2)

ada = AdaBoostClassifier(max_iter=50, verbose=False)
ada.fit(trainX, trainY)
acc = ada.score(testX, testY)
print("Accuracy:", acc)
```

&emsp;&emsp;输出为：

![5.1](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-02-29/5.1.PNG)
&emsp;&emsp;**在测试集上的准确率为1，可见AdaBoost算法为一个分类精度很高的算法。**

# 7. Reference
[1 集成学习之Adaboost算法原理小结](https://www.cnblogs.com/pinard/p/6133937.html)  
[2 决策树（中）](https://zhuanlan.zhihu.com/p/86263786)  
3 统计学习方法第八章-李航