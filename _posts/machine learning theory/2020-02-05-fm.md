---
layout: post
title: "推荐系统-FM模型理论详解"
subtitle: ''
date:     2020-02-06 13:59:00
author: "mudux"
music: false
music-id: 187937
mathjax: true
autoNumber: true
# header-img: "img/post-bg-2015.jpg"
header-style: text
catalog: true
tags:
  - machine learning
  - recommender system
---

## 1. 应用背景

&emsp;在计算广告和推荐系统中，CTR预估(click-through rate)和转化率CVR(conversion rate)估计是非常重要的一个环节。判断一个物品是否进行推荐需要根据CTR预估的点击率排序决定。业界常用的方法有人工特征工程+[LR](https://hexinlin.top/2020/02/03/logistic/)(Logistic Regression)、GBDT(Gradient Boosting Decision Tree) + LR、FM(Factorization Machine)和FFM(Field-aware Factorization Machine)等。在这些模型中，FM和FFM近年来表现突出，分别在由[Criteo](https://www.kaggle.com/c/criteo-display-ad-challenge)和[Avazu](https://www.kaggle.com/c/avazu-ctr-prediction)举办的CTR预测竞赛中夺得冠军。  
&emsp;在进行CTR预估时，除了单特征外，往往要对特征进行组合。对于特征组合来说，业界现在通用的做法主要有两大类:FM系列与Tree系列。  
&emsp;最近几年出现了许多基于FM改进的方法，如deepFM，FNN，PNN，DCN，xDeepFM等，这些后面介绍。这里主要讲解FM算法。

## 2. 预测任务

&emsp;在机器学习中，预测是一项最基本的任务，所谓的预测就是估计一个函数

$$
f:{R^n} \to T \tag{2.1}
$$

&emsp;该函数将一个$n$维的实值**特征向量**$x \in {R^n}$映射到一个**目标域**$T$，例如，对于回归$T=R$，对于分类问题$T={+,-}$。在**有监督学习**(supervised learning)场景中，通常有一个带标签的训练数据集

$$D = \left\{\left(x^{(1)},{y^{(1)}}\right),\left( x^{(1)},{y^{(1)}}\right),...,\left(x^{(m)},{y^{(m)}} \right)\right\} \tag{2.2}$$


&emsp;其中$x^{(i)} \in R^n$表示输入数据，对应样本的特征向量，$y^{(i)}$对应标签，$m$为样本的数目。

&emsp;在现实世界中，许多应用问题(如文本分析、推荐系统等)会产生高度稀疏的数据，即特征向量$x^{(i)}$中几乎所有的分量都为0。这里，以**电影评分系统**为例，给出一个关于高度稀疏数据的实例。

**例1**. 考虑电影评分系统中的数据，它的每一条记录都包含了那个用户$u \in U$在什么时候$t \in R$对那部电影$i \in I$打了多少分$r \in {1,2,3,4,5}$这样的信息。假定用户集$U$和电影集$I$分别为

$$U={Alice(A), Bob(B), Charlie(C),...}$$

$$I={TItanic(TI), Nothing Hill(NH), Star Wars(SW), Star Trek(ST),...}$$

&emsp;设观测到的数据集$S$为

$$S={(A,TI,2020-1,5), (A, NH, 2010-2, 3), (A, SW, 2010-4, 1), (B, SW, 2009-5, 4), (B, ST,2009-8, 5), (C, TI, 2009-9, 1), (C, SW, 2009-12, 5),...}$$

&emsp;利用观测数据集$S$来进行预测任务的一个实例是：估计一个函数$\mathop y\limits^\^ $来预测某个用户在某个时刻对某部电影的打分行为。

&emsp;利用观测数据集$S$构造数据样本如下图所示：

![data](https://gitee.com/alston972/MarkDownPhotos/raw/master/2020-02-05/data.PNG)


- 第一部分（对应蓝色方框）表示**当前评分用户信息**，其维度为
$$|U|$$
，该部分的分量中，当前电影评分用户所在的位置为1，其他为0。例如，在第一条观测记录中，就有
$$x_A^{(1)}=1$$
。
- 第二部分（对应红色方框）表示**当前被评分电影信息**，其维度为
$$|I|$$
，该部分的分量中，当前被评分的电影所在的位置为1，其他为0。例如，在第一条观测记录中，就有
$$x_{TI}^{(1)}=1$$
。
- 第三部分（对应黄色方框）表示**当前评分用户评分过的所有电影信息**，其维度为
$$|I|$$
，该部分的分量中，被当前用户评论过的所有电影(设个数为
$$n_I$$
)的位置为
$$\frac {1}{n_I}$$
，其他为0。例如，Alice评价过三部电影TI，NH和SW，因此就有
$$x_{TI}^{(1)}=x_{NH}^{(1)}=x_{SW}^{(1)}=\frac {1}{3}$$
。
- 第四部分（对应绿色方框）表示**评分日期信息**，其维度为1，用来表示用户评价电影的时间，表示方法是将（记录中最早的日期）2009年1月作为基数1，以后每增加1个月就加1，如2009年5月就可以折算为5。
- 第五部分（对应棕色方框）表示**当前评分用户最近评分过的一部电影的信息**，其维度为
$$|I|$$
，该部分的分量中，如当前用户评价当前电影之前还评价过其它电影，则将当前用户评价的上一部电影的位置取为1，其他为0；若当前用户评价当前电影之前没有评价过其他的电影，则所有的评分都取为0。例如，对于第二条观测记录，Alice评价NH之前评价的是TI，因此有
$$x_{TI}^{(2)}=1$$
。

&emsp;在本例中，特征向量
$$x$$
&emsp;总的维度为
$$|U|+|I|+|I|+1+|I|=|U|+3|I|+1$$
。在一个真实的电影评分系统中，用户的数目$|U|$和电影的数目$|I|$是很大的，而每个用户参与评论的电影数目则相对较小，于是，每一条记录对应的特征向量将会很稀疏。为定量刻画这种稀疏，记$N_z(x)$表示$x$中非零分量的个数，并记

$$\overline {N_z}(X) = \frac{1}{m}\sum\limits_{i = 1}^m {N_z\left( {x^{(i)}} \right)} \tag{2.3}$$

&emsp;其中$X$表示由样本按行拼接成的矩阵，即


$$
X = \left(
\begin{matrix}
{x_1^{(1)}}&{x_2^{(1)}}& \cdots &{x_n^{(1)}}\\
{x_1^{(2)}}&{x_2^{(2)}}& \cdots &{x_n^{(2)}}\\
{}&{}& \ddots &{}\\
{x_1^{(m)}}&{x_2^{(m)}}&{}&{x_n^{(m)}}
\end{matrix}
\right)_{m \times n}
\tag{2.4}$$

&emsp;则$\overline {{N_z}} (X)$表示训练集$D$中所有特征向量中非零分量数的平均值。显然，对于稀疏数据，成立

$\overline {N_z} (X) \ll n$。


## 3. 模型方程


### 3.1 二阶表达式
&emsp;在介绍FM的模型方程之前，先讲一下线性回归。对于一个给定的特征向量$x=(x_1, x_2,...,x_n)^T$，线性回归建模时采用的函数是


$$\begin{array}{lll}
\widehat y\left( x \right) &=& {w_0} + {w_1}{x_1} + {w_2}{x_2} +  \cdots  + {w_n}{x_n}\\
&=&{w_0} + \sum\limits_{i = 1}^n {w_ix_i} 
\end{array}\tag{3.1}$$

&emsp;其中，$w_0$和$\rm w=(w_1, w_2,...,w_n)^T$为模型参数。从方程易见：各特征分量$x_i$和$x_j$($i \neq j$)之间是相互孤立的，即$\widehat y(x)$中仅考虑单个的特征分量，而没有考虑特征分量之间的相互关系(interaction)。

&emsp;为了表示特征交叉，将$\widehat y(x)$改写为

$$\widehat y\left( x \right) = {w_0} + \sum\limits_{i = 1}^n {w_ix_i}  + \sum\limits_{i = 1}^{n - 1} {\sum\limits_{j = i + 1}^n {w_{ij}x_ix_j} } \tag{3.2}$$

&emsp;这样任意连个互异特征分量之间的关系也考虑进来了。不过，这种直接在$x_ix_j$前面配上一个系数$w_{ij}$的方式在稀疏数据中有一个很大的缺陷，例如在上例中，观测集$S$中没有Alice评价电影Star Trek的记录，如果直接估计Alice(A)和Star Trek(ST)之间，或者说特征分量$x_A$和$x_{ST}$之间的相互关系，显然会得到系数$w_{A,ST}=0$。即对于观测样本中未出现过交互的特征分量，不能对相应的参数进行估计。注意，在高度稀疏数据场景中，由于数据量的不足，样本中出现未交互的特征分量是很普遍的。

&emsp;为克服这个缺陷，在系数$w_{ij}$上做文章，将其表示成另外的形式。为此，针对每个维度的特征分量$x_i$，引入辅助分量

$${v_i} = {\left( {v_{i1},v_{i2},...,v_{ik}} \right)^T} \in {R^k},i = 1,2,...,n\tag{3.3}$$

&emsp;其中$k \in {N^ + }$为超参数，并将$w_{ij}$改写为

$${\widehat w_{ij}} = v_i^T{v_j} = \sum\limits_{l = 1}^k {v_{il}v_{jl}} \tag{3.4}$$

&emsp;这样就能克服上面提到的缺陷了?我们粗略地分析一下。在观测数据中，Bob(B)对Star Wars(SW)和Star Trek(ST)的评分差不多，由此可认为它们对应的辅助向量$v_{ST}$和$v_{SW}$也比较相似。从而，利用$\widehat w_{A,SW}$就可以对$\widehat w_{A,ST}$进行估计了。(当然，这里并不是说，真的要先计算出$\widehat w_{A,SW}$然后用它作为$\widehat w_{A,ST}$的近似。因为这些性质都是观测数据中内在的，我们要做的只是建好模型，让模型在训练阶段自动去学习。)

&emsp;于是，函数$\widehat y$可以进一步改写为

$$\widehat y\left( x \right) = {w_0} + \sum\limits_{i = 1}^n {w_ix_i}  + \sum\limits_{i = 1}^{n - 1} {\sum\limits_{j = i + 1}^n {\left( {v_i^T{v_j}} \right)x_ix_j} } \tag{3.5}$$

&emsp;从数学的角度来看，(3.2)和(3.5)的主要区别在哪呢？对于交叉项$x_ix_j$的系数，前者用的是$w_{ij}$，后者用的是$\widehat y_{ij}$。为更好地看清楚它们之间的关系，引入几个矩阵。

- 将${v_i}_{i=1}^n$按行拼接成的矩阵$V$

$$
V=\left(
\begin{matrix}
{v_{11}}&{v_{12}}&{\cdots}&{v_{1k}}\\
{v_{21}}&{v_{22}}&{\cdots}&{v_{2k}}\\
{}&{}&{\ddots}&{}\\
{v_{n1}}&{v_{n2}}&{\cdots}&{v_{nk}}
\end{matrix}
\right)_{n \times k} = 
\left(
\begin{matrix}
{\rm v_1^T}\\
{\rm v_2^T}\\
{\vdots}\\
{\rm v_n^T}
\end{matrix}
\right)\tag{3.6}
$$

- 交互矩阵(interaction matrix)$W$
  
$$
V=\left(
\begin{matrix}
{w_{11}}&{w_{12}}&{\cdots}&{w_{1n}}\\
{w_{21}}&{w_{22}}&{\cdots}&{w_{2n}}\\
{}&{}&{\ddots}&{}\\
{w_{n1}}&{w_{n2}}&{\cdots}&{w_{nn}}
\end{matrix}
\right)_{n \times n}\tag{3.7}
$$

- 交互矩阵$\widehat W$

$$
\begin{array}{lll}
W&=&VV^T=\left(
\begin{matrix}
{\rm v_1^T}\\
{\rm v_2^T}\\
{\vdots}\\
{\rm v_n^T}
\end{matrix}
\right)(\rm v_1 \rm v_2 \cdots \rm v_n)\\
&=&
\left(
\begin{matrix}
{\rm v_1^T \rm v_1}&{\rm v_1^T \rm v_2}&{\cdots}&{\rm v_1^T \rm v_n}\\
{\rm v_2^T \rm v_1}&{\rm v_2^T \rm v_2}&{\cdots}&{\rm v_2^T \rm v_n}\\
{}&{}&{\ddots}&{}\\
{\rm v_n^T \rm v_1}&{\rm v_n^T \rm v_2}&{\cdots}&{\rm v_n^T \rm v_n}
\end{matrix}
\right)_{n \times n}\\
&=&
\left(
\begin{matrix}
{\rm v_1^T \rm v_1}&{\widehat w_{12}}&{\cdots}&{\widehat w_{1n}}\\
{\widehat w_{21}}&{\rm v_2^T \rm v_2}&{\cdots}&{\widehat w_{2n}}\\
{}&{}&{\ddots}&{}\\
{\widehat w_{n1}}&{\widehat w_{n2}}&{\cdots}&{\widehat w_{nn}}
\end{matrix}
\right)_{n \times n}
\end{array}\tag{3.8}
$$

&emsp;由此可见，(3.2)和(3.5)分别采用了交互矩阵$W$和$\widehat W$的**非对角线**元素作为$x_ix_j$的系数。由于$\widehat W=VV^T$是一种[矩阵分解]()。因此将这种以(3.5)作为模型方程的方法称为Factorization Machines方法。

&emsp;那么$VV^T$的表达能力怎么样呢？即任意给定一个交互矩阵$\widehat W$，能否找到相应的分解矩阵$V$呢？答案是肯定的，这里需要用到一个结论“**当$k$足够大时，对于任意对称正定的实矩阵$\widehat W \in R^{n \times n}$，均存在实矩阵$V \in R^{n \times k}$，使得成立$\widehat w = VV^T$**”。

&emsp;由于(3.5)中只涉及满足$i<j$的$x_ix_j$，因此$\widehat W$的对称性没有问题，那如何保证$\widehat W$的正定性呢？注意，我们只关心互异特征分量之间的相互关系，因此$\widehat W$的对角线元素是可以任意取值的，只需将它们取得足够大（例如保证行元素满足严格对角占优），就可以保证$\widehat W$的正定性。

&emsp;理论分析中，我们要求参数$k$取得足够大，但是，在高度稀疏数据场景中，由于没有足够的样本来估计复杂的交互矩阵，因此$k$通常应取得很小。事实上，对参数$k$(亦即FM的表达能力)的限制，在一定程度上可以提高模型的泛化能力。这种能够利用$\widehat W$的低秩近似的性质也是FM的一个优势。

### 3.2 复杂度分析

&emsp;FM模型方程(3.5)中需要估计的参数包括

$$w_0 \in R, \rm \in R^n, V \in R^{n \times k}$$

&emsp;共有$1+n+nk$个参数，其中$w_0$为整体的偏置量，$\rm w$对特征向量的各个分量的强度进行建模，$V$对特征向量中任意两个分量之间的关系进行了建模。

&emsp;那么，模型方程(3.5)的**计算复杂度**如何？直接从公式来看，其计算复杂度为

$${n+(n-1)}+{\frac {n(n-1)}{2}[k+(k-1)+2]+\frac{n(n-1)}{2}-1}+2=O(kn^2)$$

&emsp;其中第一个花括号对应$\sum \limits_{i=1}^n w_ix_i$的加法和乘法操作数，第二个括号对应$\sum \limits_{i=1}^{n-1}\sum \limits_{j=i+1}^{n}(\rm v_i^T v_j)x_ix_j$的加法和乘法操作数。

&emsp;如果对(3.5)进行改写，可将其计算复杂度降成线性的$O(kn)$，具体推导过程如下。

$$
\begin{array}{ll}
&\sum \limits_{i=1}^{n-1}\sum \limits_{j=i+1}^{n}(\rm v_i^Tv_j)x_ix_j\\
=&\frac {1}{2}\left(\sum \limits_{i=1}^{n}\sum \limits_{j=1}^{n}(\rm v_i^T v_j)x_ix_j - \sum\limits_{i=1}^{n}(v_i^Tv_i)x_ix_i\right)\\
=&\frac{1}{2}\left(\sum\limits_{i=1}^n\sum\limits_{j=1}^n\sum\limits_{l=1}^kv_{il}v_{jl}x_ix_j-\sum\limits_{i=1}^n\sum\limits_{l=1}^k v_{il}^2x_i^2 \right)\\
=&\frac{1}{2}\sum\limits_{l=1}^k\left[\left(\sum\limits_{i=1}^n(v_{il}x_i)\right)^2-\sum\limits_{i=1}^nv_{il}^2x_i^2\right]
\end{array}\tag{3.9}
$$

&emsp;易见，(3.9)的计算复杂度为

$$k{[n+(n-1)+1]+[3n+(n-1)+1]+1}+(k-1)+1=O(kn)$$

&emsp;注意，在高度稀疏数据场景中，特征向量$\rm x$中绝大部分分量为零，即$N_z(\rm x)$很小，而(3.9)中关于$i$的求和只需对非零元素进行，于是，计算复杂度就变成了$O(kN_z(\rm x))$。对整个数据集$D$而言，平均复杂度就是
$$O\left( {\overline {{N_z}} (X)} \right)$$
了，所以**FM可以在线性时间训练和预测**。

### 3.3 SGD求解参数

$$\frac{\partial \widehat y(x)}{\partial \theta} 
= \left\{ 
{\begin{array}{lll}
1,\text{if $\theta$ is $w_0$}\\
{x_i},\text{if $\theta$ is $w_i$}\\
{x_i}\sum\nolimits_{j = 1}^n v_{j,f}{x_j} - v_{j,f}x_i^2,\text{if $\theta$ is $v_{i,f}$}
\end{array}} \right.\tag{3.10}
$$

&emsp;**由上式可知，$v_{i,f}$的训练只需要样本的$x_i$特征非0即可，适合于稀疏数据**。

&emsp;在适用SGD训练模型是，在每次迭代中，只需计算一次所有的$f$的$\sum \limits_{j=1}^{n}v_{j,f}x_j$就能方便得到所有$v_{i,f}$的梯度。


### 3.4 推广到多阶

&emsp;前面描述的FM模型方程(3.5)包含各个特征分量，以及任意两个特征分量之间相互关系的项。我们将这种方程称为2-way(或二阶)FM方程。接下来，我们将它进一步推广位d-way(d>=2)FM方程，即方程可以同时刻画任意$\widetilde d\left( {1 \le \widetilde d \le d} \right)$个特征分量之间的相互关系。此时，方程可以定义为

$$\widehat y(\rm x) = w_0 + \sum \limits_{i=1}^n w_ix_i + \sum \limits_{l=2}^d \left[\sum\limits_{i_1=1}^{n-(l-1)}\sum\limits_{i_2=i_1+1}^{n-(l-2)} \cdots \sum\limits_{i_l=i_{l-1}+1}^{n}\left(\mathop \Pi\limits_{j=1}^{l}x_{i_j}\right)\left(\sum\limits_{k=1}^{k_l}\mathop \Pi\limits_{j=1}^{l}v_{i_jk}^{(l)}\right)\right]\tag{3.11}
$$

&emsp;以$d=3$为例，(3.11)的第三项展开来就是

$$
\sum\limits_{i_1=1}^{n-1}\sum\limits_{i_2=i_1+1}^{n}(x_{i_1}x_{i_2})\left(\sum\limits_{k=1}^{k_2}v_{i_1k}^{(2)}v_{i_2k}^{(2)}\right)+\sum\limits_{i_1=1}^{n-2}\sum\limits_{i_2=i_1+1}^{n-1}\sum\limits_{i_3=i_2+1}^{n}(x_{i_1}x_{i_2}x_{i_3})\left(\sum\limits_{k=1}^{k_3}v_{i_1k}^{(3)}v_{i_2k}^{(3)}v_{i_3k}^{(3)}\right)\tag{3.12}
$$

&emsp;上式中第一项刻画的是任意两个特征分量之间的相互关系，第二项刻画的是任意三个特征分量之间的相互关系。事实上，(3.11)中每一个$l$值对应一级特征分量相互关系，第$l(l \ge 2)$级关系的参数来自矩阵

$$V^{(l)} \in R^{n \times k_l}，k_l \in N^+\tag{3.13}$$

&emsp;易知，公式(3.11)的计算复杂度为$O(k_dn^d)$。

### 3.4 总结

FM目的：旨在解决稀疏数据下的特征组合问题。  
优势：
- 适用于高度稀疏数据场景；
- 具有线性的计算复杂度。

## 4. 回归和分类

&emsp;利用FM的模型方程，可以进行各种预测任务，如**回归(Regression)、分类(Classification)和排名(Ranking)**等。下面以回归和二分类为例，给出这两种预测任务中优化目标函数的构造。优化目标函数通常取为形如

$$L=\sum \limits_{i=1}^mloss(\widehat y(\rm x^{(i)}), y^{(i)})\tag{4.1}$$

的损失函数，优化的目标是将其最小化，其中$loss(\widehat y(\rm x^{(i)}), y^{(i)})$称为关于第$i$个样本$(x^{(i)}, y^{(i)})$的损失函数。

- **回归问题**  
对于回归问题，损失函数可取为**最小平方误差**(least square error),即

$$loss(y, \widehat y)=(\widehat y - y)^2\tag{4.2}$$

- **二分类问题**  
对于二分类(Binary Classification)问题(其中**标签$y \in {+1, -1}$)**，损失函数可取为**hinge loss函数**或**logit 函数**。
1. **hinge loss函数**
   
   $$loss(\widehat y, y)=max{0, 1-\widehat y y}\tag{4.3}$$

   当$y=+1$时

   $$loss(\widehat y, y)=max{0, 1-\widehat y}
   =\left \{
     {\begin{array}{lll}
     0, \widehat y \ge 1;\\
     1-\widehat y, \widehat y \lt 1,
     \end{array}}
     \right.\tag{4.4}$$

    
  
   当$y=-1$时

   $$loss(\widehat y, y)=max{0, 1+\widehat y}
   =\left \{
     {\begin{array}{lll}
     0, \widehat y \le -1;\\
     1+\widehat y, \widehat y \gt -1,
     \end{array}}
     \right.\tag{4.5}$$

    模型训练好后，就可以利用$\widehat y(\rm x)$的正负符号来预测$\rm x$的分类了。     
2. **logit loss函数**

  $$loss(\widehat y, y)=-\ln \sigma(\widehat yy)\tag{4.6}$$

  &emsp;其中$\sigma (x)=\frac {1}{1+e^{-x}}$为sigmoid函数。由定义(4.6)可见，$\widehat y$和$y$越接近，损失$$loss(\widehat y, y)$$就越小。

  此外，为了防止过拟合，通常会在优化目标函数加入**正则项**。

## 5. 其他

### 5.1 关于隐向量V

&emsp;这里的$v_i$是$x_i$特征的低维稠密表达，实际中隐向量的长度$k$通常远小于特征维度$n$。FM学到的隐向量可以看做是特征的一种embedding表示，把离散特征转化为Dense Feature，这种Dense Feature还可以后续和DNN结合，作为DNN的输入，事实上用于DNN的CTR也是这个思路来做的。

## 6. 后续  
FM代码实现。

## Reference
[分解机(Factorization Machines)推荐算法原理-刘建平](https://www.cnblogs.com/pinard/p/6370127.html)  
[FM（Factorization Machines）的理论与实践-知乎](https://zhuanlan.zhihu.com/p/50426292)  
[FM-pytorch](http://shomy.top/2018/12/31/factorization-machine/)  
[深入FFM原理与实践-美团技术团队](https://tech.meituan.com/2016/03/03/deep-understanding-of-ffm-principles-and-practices.html)  
[princewen-github](https://github.com/princewen/tensorflow_practice/tree/master/recommendation)  
[FM算法解析](https://zhuanlan.zhihu.com/p/37963267)  
[FM-github](https://github.com/Aifcce/FM-FFM)  